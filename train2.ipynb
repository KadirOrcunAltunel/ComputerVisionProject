{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "Z5VuAlC3R0yh",
   "metadata": {
    "id": "Z5VuAlC3R0yh"
   },
   "source": [
    "# Computer Vision Analysis\n",
    "### Adding RetinaNet to our model\n",
    "#### By Ronny Toribio, Kadir O. Altunel, Michael Cook-Stahl\n",
    "#### Based on [Hands on Machine Learning 2nd edition](https://github.com/ageron/handson-ml2/), [FER2013 candidate 1](https://www.kaggle.com/code/ritikjain00/model-training-fer-13) and [FER2013 candidate 2](https://www.kaggle.com/code/gauravsharma99/facial-emotion-recognition/notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t9BVYDwBR0ym",
   "metadata": {
    "id": "t9BVYDwBR0ym"
   },
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FrDuFczgR0yn",
   "metadata": {
    "id": "FrDuFczgR0yn"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import os.path\n",
    "import shutil\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model, Sequential, model_from_json\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense, BatchNormalization, Layer, UpSampling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, CSVLogger, ModelCheckpoint \n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0VXelRyIR0yp",
   "metadata": {
    "id": "0VXelRyIR0yp"
   },
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YX16VTJoR0yq",
   "metadata": {
    "id": "YX16VTJoR0yq"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMAGE_ZOOM = 0.3\n",
    "IMAGE_SHAPE = (48, 48)\n",
    "INPUT_SHAPE = (48, 48, 1)\n",
    "TRAIN_DIR = os.path.join(\"fer2013\", \"train\")\n",
    "TEST_DIR = os.path.join(\"fer2013\", \"test\")\n",
    "GOOGLE_DRIVE_PATH = \"/content/drive/MyDrive/Machine Learning/GridSearch\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9b4e67",
   "metadata": {},
   "source": [
    "### Utility functions (Bounding Boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6cf6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_xy(boxes):\n",
    "    return tf.stack([boxes[:, 1], boxes[:, 0], boxes[:, 3], boxes[:, 2]], axis=-1)\n",
    "\n",
    "\n",
    "def convert_to_xywh(boxes):\n",
    "    return tf.concat(\n",
    "        [(boxes[..., :2] + boxes[..., 2:]) / 2.0, boxes[..., 2:] - boxes[..., :2]],\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "def convert_to_corners(boxes):\n",
    "    return tf.concat(\n",
    "        [boxes[..., :2] - boxes[..., 2:] / 2.0, boxes[..., :2] + boxes[..., 2:] / 2.0],\n",
    "        axis=-1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0379a2",
   "metadata": {},
   "source": [
    "### Computing pairwise Intersection Over Union (Bounding Boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39375deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(boxes1, boxes2):\n",
    "    boxes1_corners = convert_to_corners(boxes1)\n",
    "    boxes2_corners = convert_to_corners(boxes2)\n",
    "    lu = tf.maximum(boxes1_corners[:, None, :2], boxes2_corners[:, :2])\n",
    "    rd = tf.minimum(boxes1_corners[:, None, 2:], boxes2_corners[:, 2:])\n",
    "    intersection = tf.maximum(0.0, rd - lu)\n",
    "    intersection_area = intersection[:, :, 0] * intersection[:, :, 1]\n",
    "    boxes1_area = boxes1[:, 2] * boxes1[:, 3]\n",
    "    boxes2_area = boxes2[:, 2] * boxes2[:, 3]\n",
    "    union_area = tf.maximum(\n",
    "        boxes1_area[:, None] + boxes2_area - intersection_area, 1e-8\n",
    "    )\n",
    "    return tf.clip_by_value(intersection_area / union_area, 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decaa90a",
   "metadata": {},
   "source": [
    "### AnchorBox (Bounding Boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2746e9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnchorBox:\n",
    "    def __init__(self):\n",
    "        self.aspect_ratios = [0.5, 1.0, 2.0]\n",
    "        self.scales = [2 ** x for x in [0, 1 / 3, 2 / 3]]\n",
    "        self._num_anchors = len(self.aspect_ratios) * len(self.scales)\n",
    "        self._strides = [2 ** i for i in range(3, 8)]\n",
    "        self._areas = [x ** 2 for x in [32.0, 64.0, 128.0, 256.0, 512.0]]\n",
    "        self._anchor_dims = self._compute_dims()\n",
    "\n",
    "    def _compute_dims(self):\n",
    "        anchor_dims_all = []\n",
    "        for area in self._areas:\n",
    "            anchor_dims = []\n",
    "            for ratio in self.aspect_ratios:\n",
    "                anchor_height = tf.math.sqrt(area / ratio)\n",
    "                anchor_width = area / anchor_height\n",
    "                dims = tf.reshape(\n",
    "                    tf.stack([anchor_width, anchor_height], axis=-1), [1, 1, 2]\n",
    "                )\n",
    "                for scale in self.scales:\n",
    "                    anchor_dims.append(scale * dims)\n",
    "            anchor_dims_all.append(tf.stack(anchor_dims, axis=-2))\n",
    "        return anchor_dims_all\n",
    "\n",
    "    def _get_anchors(self, feature_height, feature_width, level):\n",
    "        rx = tf.range(feature_width, dtype=tf.float32) + 0.5\n",
    "        ry = tf.range(feature_height, dtype=tf.float32) + 0.5\n",
    "        centers = tf.stack(tf.meshgrid(rx, ry), axis=-1) * self._strides[level - 3]\n",
    "        centers = tf.expand_dims(centers, axis=-2)\n",
    "        centers = tf.tile(centers, [1, 1, self._num_anchors, 1])\n",
    "        dims = tf.tile(\n",
    "            self._anchor_dims[level - 3], [feature_height, feature_width, 1, 1]\n",
    "        )\n",
    "        anchors = tf.concat([centers, dims], axis=-1)\n",
    "        return tf.reshape(\n",
    "            anchors, [feature_height * feature_width * self._num_anchors, 4]\n",
    "        )\n",
    "\n",
    "    def get_anchors(self, image_height, image_width):\n",
    "        anchors = [\n",
    "            self._get_anchors(\n",
    "                tf.math.ceil(image_height / 2 ** i),\n",
    "                tf.math.ceil(image_width / 2 ** i),\n",
    "                i,\n",
    "            )\n",
    "            for i in range(3, 8)\n",
    "        ]\n",
    "        return tf.concat(anchors, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2838d4",
   "metadata": {},
   "source": [
    "### Build Head Function (Bounding Boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52519c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_head(output_filters, bias_init):\n",
    "    head = Sequential([tensorflow.keras.Input(shape=[None, None, 256])])\n",
    "    kernel_init = tf.initializers.RandomNormal(0.0, 0.01)\n",
    "    for _ in range(4):\n",
    "        head.add(Conv2D(256, kernel_size=(3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=kernel_init))\n",
    "    head.add(Conv2D(output_filters, 3, 1, padding=\"same\", kernel_initializer=kernel_init, bias_initializer=bias_init,))\n",
    "    return head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23558cdc",
   "metadata": {},
   "source": [
    "### Feature Pyramid Network as a Keras Layer (Bounding Boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0931c871",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturePyramid(Layer):\n",
    "    def __init__(self, backbone=None, **kwargs):\n",
    "        super(FeaturePyramid, self).__init__(name=\"FeaturePyramid\", **kwargs)\n",
    "        self.backbone = backbone if backbone else get_backbone()\n",
    "        self.conv_c3_1x1 = Conv2D(256, 1, 1, \"same\")\n",
    "        self.conv_c4_1x1 = Conv2D(256, 1, 1, \"same\")\n",
    "        self.conv_c5_1x1 = Conv2D(256, 1, 1, \"same\")\n",
    "        self.conv_c3_3x3 = Conv2D(256, 3, 1, \"same\")\n",
    "        self.conv_c4_3x3 = Conv2D(256, 3, 1, \"same\")\n",
    "        self.conv_c5_3x3 = Conv2D(256, 3, 1, \"same\")\n",
    "        self.conv_c6_3x3 = Conv2D(256, 3, 2, \"same\")\n",
    "        self.conv_c7_3x3 = Conv2D(256, 3, 2, \"same\")\n",
    "        self.upsample_2x = UpSampling2D(2)\n",
    "\n",
    "    def call(self, images, training=False):\n",
    "        c3_output, c4_output, c5_output = self.backbone(images, training=training)\n",
    "        p3_output = self.conv_c3_1x1(c3_output)\n",
    "        p4_output = self.conv_c4_1x1(c4_output)\n",
    "        p5_output = self.conv_c5_1x1(c5_output)\n",
    "        p4_output = p4_output + self.upsample_2x(p5_output)\n",
    "        p3_output = p3_output + self.upsample_2x(p4_output)\n",
    "        p3_output = self.conv_c3_3x3(p3_output)\n",
    "        p4_output = self.conv_c4_3x3(p4_output)\n",
    "        p5_output = self.conv_c5_3x3(p5_output)\n",
    "        p6_output = self.conv_c6_3x3(c5_output)\n",
    "        p7_output = self.conv_c7_3x3(tf.nn.relu(p6_output))\n",
    "        return p3_output, p4_output, p5_output, p6_output, p7_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4c5cf0",
   "metadata": {},
   "source": [
    "### LabelEncoder (Bounding Boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc2903f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelEncoder:\n",
    "    def __init__(self):\n",
    "        self._anchor_box = AnchorBox()\n",
    "        self._box_variance = tf.convert_to_tensor(\n",
    "            [0.1, 0.1, 0.2, 0.2], dtype=tf.float32\n",
    "        )\n",
    "\n",
    "    def _match_anchor_boxes(\n",
    "        self, anchor_boxes, gt_boxes, match_iou=0.5, ignore_iou=0.4\n",
    "    ):\n",
    "        iou_matrix = compute_iou(anchor_boxes, gt_boxes)\n",
    "        max_iou = tf.reduce_max(iou_matrix, axis=1)\n",
    "        matched_gt_idx = tf.argmax(iou_matrix, axis=1)\n",
    "        positive_mask = tf.greater_equal(max_iou, match_iou)\n",
    "        negative_mask = tf.less(max_iou, ignore_iou)\n",
    "        ignore_mask = tf.logical_not(tf.logical_or(positive_mask, negative_mask))\n",
    "        return (\n",
    "            matched_gt_idx,\n",
    "            tf.cast(positive_mask, dtype=tf.float32),\n",
    "            tf.cast(ignore_mask, dtype=tf.float32),\n",
    "        )\n",
    "\n",
    "    def _compute_box_target(self, anchor_boxes, matched_gt_boxes):\n",
    "        box_target = tf.concat(\n",
    "            [\n",
    "                (matched_gt_boxes[:, :2] - anchor_boxes[:, :2]) / anchor_boxes[:, 2:],\n",
    "                tf.math.log(matched_gt_boxes[:, 2:] / anchor_boxes[:, 2:]),\n",
    "            ],\n",
    "            axis=-1,\n",
    "        )\n",
    "        box_target = box_target / self._box_variance\n",
    "        return box_target\n",
    "\n",
    "    def _encode_sample(self, image_shape, gt_boxes, cls_ids):\n",
    "        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n",
    "        cls_ids = tf.cast(cls_ids, dtype=tf.float32)\n",
    "        matched_gt_idx, positive_mask, ignore_mask = self._match_anchor_boxes(anchor_boxes, gt_boxes)\n",
    "        matched_gt_boxes = tf.gather(gt_boxes, matched_gt_idx)\n",
    "        box_target = self._compute_box_target(anchor_boxes, matched_gt_boxes)\n",
    "        matched_gt_cls_ids = tf.gather(cls_ids, matched_gt_idx)\n",
    "        cls_target = tf.where(tf.not_equal(positive_mask, 1.0), -1.0, matched_gt_cls_ids)\n",
    "        cls_target = tf.where(tf.equal(ignore_mask, 1.0), -2.0, cls_target)\n",
    "        cls_target = tf.expand_dims(cls_target, axis=-1)\n",
    "        label = tf.concat([box_target, cls_target], axis=-1)\n",
    "        return label\n",
    "\n",
    "    def encode_batch(self, batch_images, gt_boxes, cls_ids):\n",
    "        images_shape = tf.shape(batch_images)\n",
    "        batch_size = images_shape[0]\n",
    "        labels = tf.TensorArray(dtype=tf.float32, size=batch_size, dynamic_size=True)\n",
    "        for i in range(batch_size):\n",
    "            label = self._encode_sample(images_shape, gt_boxes[i], cls_ids[i])\n",
    "            labels = labels.write(i, label)\n",
    "        batch_images = tf.keras.applications.resnet.preprocess_input(batch_images)\n",
    "        return batch_images, labels.stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45594d2e",
   "metadata": {},
   "source": [
    "### Retinanet (Bounding Boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026d6e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetinaNet(Model):\n",
    "    def __init__(self, num_classes, backbone=None, **kwargs):\n",
    "        super(RetinaNet, self).__init__(name=\"RetinaNet\", **kwargs)\n",
    "        self.fpn = FeaturePyramid(backbone)\n",
    "        self.num_classes = num_classes\n",
    "        prior_probability = tf.constant_initializer(-np.log((1 - 0.01) / 0.01))\n",
    "        self.cls_head = build_head(9 * num_classes, prior_probability)\n",
    "        self.box_head = build_head(9 * 4, \"zeros\")\n",
    "\n",
    "    def call(self, image, training=False):\n",
    "        features = self.fpn(image, training=training)\n",
    "        N = tf.shape(image)[0]\n",
    "        cls_outputs = []\n",
    "        box_outputs = []\n",
    "        for feature in features:\n",
    "            box_outputs.append(tf.reshape(self.box_head(feature), [N, -1, 4]))\n",
    "            cls_outputs.append(tf.reshape(self.cls_head(feature), [N, -1, self.num_classes]))\n",
    "        cls_outputs = tf.concat(cls_outputs, axis=1)\n",
    "        box_outputs = tf.concat(box_outputs, axis=1)\n",
    "        return tf.concat([box_outputs, cls_outputs], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e096918b",
   "metadata": {},
   "source": [
    "### RetinaNetBoxLoss (Bounding Boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32af0dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetinaNetBoxLoss(tf.losses.Loss):\n",
    "    def __init__(self, delta):\n",
    "        super(RetinaNetBoxLoss, self).__init__(reduction=\"none\", name=\"RetinaNetBoxLoss\")\n",
    "        self._delta = delta\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        difference = y_true - y_pred\n",
    "        absolute_difference = tf.abs(difference)\n",
    "        squared_difference = difference ** 2\n",
    "        loss = tf.where(\n",
    "            tf.less(absolute_difference, self._delta),\n",
    "            0.5 * squared_difference,\n",
    "            absolute_difference - 0.5,\n",
    "        )\n",
    "        return tf.reduce_sum(loss, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d34923a",
   "metadata": {},
   "source": [
    "### RetinaNetClassificationLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3bf50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetinaNetClassificationLoss(tf.losses.Loss):\n",
    "    def __init__(self, alpha, gamma):\n",
    "        super(RetinaNetClassificationLoss, self).__init__(reduction=\"none\", name=\"RetinaNetClassificationLoss\")\n",
    "        self._alpha = alpha\n",
    "        self._gamma = gamma\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "        probs = tf.nn.sigmoid(y_pred)\n",
    "        alpha = tf.where(tf.equal(y_true, 1.0), self._alpha, (1.0 - self._alpha))\n",
    "        pt = tf.where(tf.equal(y_true, 1.0), probs, 1 - probs)\n",
    "        loss = alpha * tf.pow(1.0 - pt, self._gamma) * cross_entropy\n",
    "        return tf.reduce_sum(loss, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920cff13",
   "metadata": {},
   "source": [
    "### RetinaNetLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e34520",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetinaNetLoss(tf.losses.Loss):\n",
    "    def __init__(self, num_classes=80, alpha=0.25, gamma=2.0, delta=1.0):\n",
    "        super(RetinaNetLoss, self).__init__(reduction=\"auto\", name=\"RetinaNetLoss\")\n",
    "        self._clf_loss = RetinaNetClassificationLoss(alpha, gamma)\n",
    "        self._box_loss = RetinaNetBoxLoss(delta)\n",
    "        self._num_classes = num_classes\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "        box_labels = y_true[:, :, :4]\n",
    "        box_predictions = y_pred[:, :, :4]\n",
    "        cls_labels = tf.one_hot(\n",
    "            tf.cast(y_true[:, :, 4], dtype=tf.int32),\n",
    "            depth=self._num_classes,\n",
    "            dtype=tf.float32,\n",
    "        )\n",
    "        cls_predictions = y_pred[:, :, 4:]\n",
    "        positive_mask = tf.cast(tf.greater(y_true[:, :, 4], -1.0), dtype=tf.float32)\n",
    "        ignore_mask = tf.cast(tf.equal(y_true[:, :, 4], -2.0), dtype=tf.float32)\n",
    "        clf_loss = self._clf_loss(cls_labels, cls_predictions)\n",
    "        box_loss = self._box_loss(box_labels, box_predictions)\n",
    "        clf_loss = tf.where(tf.equal(ignore_mask, 1.0), 0.0, clf_loss)\n",
    "        box_loss = tf.where(tf.equal(positive_mask, 1.0), box_loss, 0.0)\n",
    "        normalizer = tf.reduce_sum(positive_mask, axis=-1)\n",
    "        clf_loss = tf.math.divide_no_nan(tf.reduce_sum(clf_loss, axis=-1), normalizer)\n",
    "        box_loss = tf.math.divide_no_nan(tf.reduce_sum(box_loss, axis=-1), normalizer)\n",
    "        loss = clf_loss + box_loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z58JCINW9oFx",
   "metadata": {
    "id": "z58JCINW9oFx"
   },
   "source": [
    "### Mount Google Drive and unzip dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xDczGY4M9zjB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xDczGY4M9zjB",
    "outputId": "6c7900bc-dce4-44be-e1aa-6d241216d24b"
   },
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')\n",
    "os.makedirs(GOOGLE_DRIVE_PATH, exist_ok=True)\n",
    "!unzip /content/drive/MyDrive/FER.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TjEEOXKRR0yq",
   "metadata": {
    "id": "TjEEOXKRR0yq"
   },
   "source": [
    "### Load Facial Emotion Recognition dataset\n",
    "#### training, validation, and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hWfl8NDJR0yr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hWfl8NDJR0yr",
    "outputId": "854137aa-574b-4de6-dfee-8c74384ac8bc"
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, zoom_range=IMAGE_ZOOM, horizontal_flip=True,\n",
    "                                   validation_split=0.10)\n",
    "Xy_train = train_datagen.flow_from_directory(TRAIN_DIR, batch_size=BATCH_SIZE, \n",
    "                                   target_size=IMAGE_SHAPE, shuffle=True, subset=\"training\",\n",
    "                                   color_mode=\"grayscale\", class_mode=\"categorical\")\n",
    "\n",
    "Xy_valid = train_datagen.flow_from_directory(TRAIN_DIR, batch_size=BATCH_SIZE, \n",
    "                                   target_size=IMAGE_SHAPE, shuffle=True, subset=\"validation\",\n",
    "                                   color_mode=\"grayscale\", class_mode=\"categorical\")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "Xy_test = test_datagen.flow_from_directory(TEST_DIR, batch_size=BATCH_SIZE,\n",
    "                                   target_size=IMAGE_SHAPE, shuffle=True,\n",
    "                                   color_mode=\"grayscale\", class_mode=\"categorical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pgUbpnyxR0ys",
   "metadata": {
    "id": "pgUbpnyxR0ys"
   },
   "source": [
    "### Load our CNN model from JSON and HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BIqKQeCLR0ys",
   "metadata": {
    "id": "BIqKQeCLR0ys"
   },
   "outputs": [],
   "source": [
    "with open(\"cnn_model.json\", \"r\") as json_model_file:\n",
    "    json_model_str = json_model_file.read()\n",
    "model = model_from_json(json_model_str)\n",
    "model.load_weights(\"cnn_model_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430f0d6c",
   "metadata": {},
   "source": [
    "### Remove classification block layers and freeze the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7e71ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(model.layers[:-5])\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45aed040",
   "metadata": {},
   "source": [
    "### Attach RetinaNet to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ccafda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RetinaNet(7, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40243cb",
   "metadata": {},
   "source": [
    "### Compile our new CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b63b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = RetinaNetLoss(7)\n",
    "learning_rates = [2.5e-06, 0.000625, 0.00125, 0.0025, 0.00025, 2.5e-05]\n",
    "learning_rate_boundaries = [125, 250, 500, 240000, 360000]\n",
    "learning_rate_fn = tf.optimizers.schedules.PiecewiseConstantDecay(\n",
    "    boundaries=learning_rate_boundaries, values=learning_rates\n",
    ")\n",
    "\n",
    "model.compile(loss=loss_fn, optimizer=Nadam(learning_rate=learning_rate_fn, beta_1=0.9, epsilon=1e-07),\n",
    "    metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4142d6b",
   "metadata": {},
   "source": [
    "### Save our new CNN model architecture as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c619e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json2 = model.to_json()\n",
    "with open(\"cnn_model2.json\") as json_model_file2:\n",
    "    f.write(model_json2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52684254",
   "metadata": {},
   "source": [
    "### Model Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f47f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb = ModelCheckpoint(\"cnn_model2_weights.h5\", monitor = 'accuracy', verbose =1, \n",
    "                                save_best_only = True, save_weights_only = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kn0Z_MmHR0yt",
   "metadata": {
    "id": "kn0Z_MmHR0yt"
   },
   "source": [
    "### Early stopping callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p6bwkyo5R0yu",
   "metadata": {
    "id": "p6bwkyo5R0yu"
   },
   "outputs": [],
   "source": [
    "early_stopping_cb = EarlyStopping(min_delta=0.00005, patience=11, verbose=1, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tBLRTBV_R0yu",
   "metadata": {
    "id": "tBLRTBV_R0yu"
   },
   "source": [
    "### Reduce learning rate on plateau callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JHdcC0DaR0yv",
   "metadata": {
    "id": "JHdcC0DaR0yv"
   },
   "outputs": [],
   "source": [
    "reduce_lr_cb = ReduceLROnPlateau(factor=0.5, patience=7, min_l=1e-7, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1fce1a",
   "metadata": {},
   "source": [
    "### CSV Logger Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0592b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_cb = CSVLogger(\"cnn_model2_training.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QC6W089cR0yv",
   "metadata": {
    "id": "QC6W089cR0yv"
   },
   "source": [
    "### Train our new CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4wdiDF4qR0yv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4wdiDF4qR0yv",
    "outputId": "bb0e630d-0a3c-41c8-85c4-c91cec76560b"
   },
   "outputs": [],
   "source": [
    "history = cur_model.fit(Xy_train, epochs=80,\n",
    "                        validation_data=(Xy_valid),\n",
    "                        steps_per_epoch=Xy_train.n // BATCH_SIZE,\n",
    "                        validation_steps=Xy_valid.n // BATCH_SIZE,\n",
    "                        callbacks=[early_stopping_cb, reduce_lr_cb, csv_cb, checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362cfdc2",
   "metadata": {},
   "source": [
    "### Evaluate new CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588082cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluate train set\")\n",
    "model.evaluate(Xy_train)\n",
    "print(\"Evaluate validation set\")\n",
    "model.evaluate(Xy_valid)\n",
    "print(\"Evaluate test set\")\n",
    "model.evaluate(Xy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e551f26c",
   "metadata": {
    "id": "e551f26c"
   },
   "source": [
    "### Convert histories into graph images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f1e0d9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "82f1e0d9",
    "outputId": "ca73797d-5b9d-49cb-ebfb-ea8e5ee4d619"
   },
   "outputs": [],
   "source": [
    "training = pd.read_csv(\"cnn_model2_training.csv\", index_col=\"epoch\")\n",
    "fig, ax1 = plt.subplot(1, 1)\n",
    "ax2 = ax1.twinx()\n",
    "ax1.set_xlabel(\"Epochs\")\n",
    "ax1.set_ylabel(\"Accuracy\", color=\"b\")\n",
    "ax2.set_ylabel(\"Loss\", color=\"y\")\n",
    "ax1.plot(training[\"epoch\"], training[\"accuracy\"], \"b-\", label=\"accuracy\")\n",
    "ax1.plot(training[\"epoch\"], training[\"val_accuracy\"], \"r-\", label=\"val_accuracy\")\n",
    "ax1.plot(training[\"epoch\"], training[\"lr\"], \"g-\", label=\"lr\")\n",
    "ax2.plot(training[\"epoch\"], training[\"loss\"], \"y-\", label=\"loss\")\n",
    "ax2.plot(training[\"epoch\"], training[\"val_loss\"], \"m-\", label=\"val_loss\")\n",
    "h1, l1 = ax1.get_legend_handles_labels()\n",
    "h2, l2 = ax2.get_legend_handles_labels()\n",
    "plt.legend(h1+h2, l1+l2)\n",
    "plt.title(name)\n",
    "plt.savefig(name + \".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fa1d06",
   "metadata": {
    "id": "42fa1d06"
   },
   "source": [
    "### Copy models to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cEpYWIrVMe3X",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "cEpYWIrVMe3X",
    "outputId": "72976bb4-60cc-49c4-d3d6-44b710283b24"
   },
   "outputs": [],
   "source": [
    "for f in os.listdir(\".\"):\n",
    "    shutil.copy(f, GOOGLE_DRIVE_PATH)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "machine_shape": "hm",
   "name": "train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
